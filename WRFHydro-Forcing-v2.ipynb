{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect NLDAS Forcing Data\n",
    "\n",
    "This notebook demonstrates the process of collecting NLDAS forcing data, regridding these data onto an existing WRF-Hydro (domain collected in the previous notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Objective\n",
    "\n",
    "This notebook illustrates the process of collecting and regridding NLDAS data for a WRF-Hydro domain. This include:\n",
    "\n",
    "1. Subsetting NLDAS data from Earthdata.nasa.gov \n",
    "2. Downloading NLDAS data\n",
    "3. Regridding NLDAS using the ESMF regridding scripts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install python3-wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import wget\n",
    "import time\n",
    "import glob\n",
    "import shutil\n",
    "import tarfile\n",
    "import getpass \n",
    "import urllib.parse as p\n",
    "import multiprocessing as mp\n",
    "from urllib.request import urlopen\n",
    "from subprocess import Popen, PIPE, STDOUT, check_output, CalledProcessError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that the NCAR Command Language is installed. Detailed instructions can be found [here](https://www.ncl.ucar.edu/Download/conda.shtml). This following cell will install `ncl` into your conda environment if it doesn't already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure we invoke the NCL that is installed in our conda environment\n",
    "ncl_exec = os.path.join(os.path.dirname(sys.executable), 'ncl')\n",
    "try:\n",
    "    output = check_output([ncl_exec, '-V'])\n",
    "    print('NCL is already installed :)')\n",
    "except Exception as e:\n",
    "    !conda install -y ncl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Subset NLDAS data from Earthdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See this [link](https://hydro1.gesdisc.eosdis.nasa.gov/data/NLDAS/README.NLDAS2.pdf) for details about primary (FORA) and secondary (FORB) NLDAS forcing datasets. The following steps are completed in the https://urs.earthdata.nasa.gov/ website:\n",
    "\n",
    "1. Create Earthdata account: [link](https://urs.earthdata.nasa.gov/users/new)\n",
    "2. Link GES DISC with your earthdata account: [link](https://disc.gsfc.nasa.gov/earthdata-login)\n",
    "3. Select NLDAS2 data via: [https://disc.gsfc.nasa.gov/datasets/NLDAS_FORA0125_H_V002/summary?keywords=NLDAS](https://disc.gsfc.nasa.gov/datasets/NLDAS_FORA0125_H_V002/summary?keywords=NLDAS) and subset the data to a reasonable size and date range using the `Simple Subset Wizard`.  \n",
    "5. Specify the date range   \n",
    "6. Use the 'Spatial Bounding Box' to trace a rough area of where your watershed exists. (This DOES NOT need to be exact!)\n",
    "7. Choose the following variables to extract:\n",
    "    - 10-m above ground Meridional wind speed\n",
    "    - 10-m above ground Zonal wind speed \n",
    "    - 2-m above ground Specific humidity\n",
    "    - 2-m above ground Temperature \n",
    "    - LW radiation flux downwards\n",
    "    - Precipitation hourly total\n",
    "    - Surface pressure\n",
    "    - SW radiation flux downwards\n",
    "8. Select file type `GRIB` from the dropdown menue\n",
    "9. Click \"Subset Selected Data Sets\" \n",
    "10. Click \"View Subset Results\"\n",
    "11. Above the list of files, select the \"list of URLs\"  \n",
    "12. Copy the URL in the address bar at the top of the page with the URL list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the URL for downloading the subsetted NLDAS data, from the previous step, below. For example, https://disc.gsfc.nasa.gov/SSW/WWW-TMP/SSW_download_2019-06-19T18:32:48_60480_NukeoWYr.inp. This URL will be used to write the link addresses for each NLDAS dataset to a local file.  \n",
    "\n",
    "Run the following code block and you will be prompted to paste in your URL for the Forcing from the last step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_urls = input(\"Enter EarthData Download URL: \")\n",
    "\n",
    "# read all of the urls defined in the link above\n",
    "input_dir = 'input_files'\n",
    "if os.path.exists(input_dir):\n",
    "    while 1:\n",
    "        res = input(f\"Directory '{input_dir}' already exists. Do you wish to remove it [Y/N]?\")\n",
    "        if res.lower() == 'y':\n",
    "            shutil.rmtree(input_dir)\n",
    "            os.mkdir(input_dir)\n",
    "            break\n",
    "        elif res.lower() == 'n':\n",
    "            break\n",
    "        else:\n",
    "            print('\\nInvalid input. Please answer either Y or N')\n",
    "else:\n",
    "    os.mkdir(input_dir)\n",
    "\n",
    "# write these urls to a local file\n",
    "print(f'Writing NLDAS urls to {input_dir}/urls.txt')\n",
    "f = urlopen(file_urls)\n",
    "urls = f.read().decode('utf-8')\n",
    "with open('input_files/urls.txt', 'w') as f:\n",
    "    f.write(urls)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the files listed in `input_files/urls.txt` using `wget`. This may take several minutes depending on how many files you're downloading. This following command can be executed in a separate terminal window to watch the files as they're downloaded:\n",
    "\n",
    "```\n",
    "$watch \"ls -l <path to data>/input_files | wc -l\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to download all urls in our file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to download nwm results\n",
    "def get_nldas(q, iolock, out_q, cnt):\n",
    "    while True:\n",
    "        user, pwd, url, total = q.get()        \n",
    "        if url is None:\n",
    "            break\n",
    "        outfile = os.path.join('input_files', url.split('LABEL=')[-1].split('&')[0])\n",
    "        !wget --quiet \\\n",
    "              --auth-no-challenge=on \\\n",
    "              --content-disposition \\\n",
    "              --user {user} \\\n",
    "              --password {pwd} \\\n",
    "              -O {outfile} \\\n",
    "              \"{url}\"\n",
    "        with iolock:\n",
    "            cnt.value += 1\n",
    "            percent = cnt.value / total * 100\n",
    "            print(f'\\r[{cnt.value} of {total}] files downloaded -- {percent:.2f}% complete', end=10*' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invoke this function in parallel to speed up the download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user = input(\"Earthdata User: \")\n",
    "password = getpass.getpass(\"Earthdata Password: \")\n",
    "\n",
    "NCORE = 4\n",
    "in_q = mp.Queue(maxsize=NCORE)\n",
    "out_q = mp.Queue()\n",
    "cnt = mp.Value('i', 0)\n",
    "iolock = mp.Lock()\n",
    "\n",
    "pool = mp.Pool(NCORE, initializer=get_nldas, \n",
    "               initargs=(in_q, iolock, out_q, cnt))\n",
    "\n",
    "with open('input_files/urls.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    total = len(lines)\n",
    "    for url in lines:\n",
    "        time.sleep(.1)\n",
    "        in_q.put((user, password, url, total))  # blocks until q below its max size\n",
    "for _ in range(NCORE):  # tell workers we're done\n",
    "    in_q.put((None, None, None, None))\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preview the data that we downloaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "du -h input_files/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data that we've downloaded are NLDAS NetCDF files that have been subsetted through time, space, and variable. These data still need to be regridded to the WRF-Hydro domain that was collected in the previous notebook.\n",
    "\n",
    "Create regridding 'weight' files required by the ESMF regridders. The weight files are netCDF files which specify interpolation weights between the source coordinate data grids (src) and destination coordinate data (dst) grids. The weight file is generated by running the `NLDAS2WRFHydro_generate_weights.ncl` script. We'll need to provide the source and destination grid filenames as arguments to the script, for example:\n",
    "\n",
    "```\n",
    "$ ncl interp_opt=\"bilinear\"\n",
    "    srcGridName=<NLDAS NetCDF File>\n",
    "    dstGridName=<WRF-Hydro geo_em.d01.nc>\n",
    "    NLDAS2WRFHydro_generate_weights.ncl\n",
    "```\n",
    "\n",
    "This will create the following files:\n",
    "\n",
    "```\n",
    "DAS2WRFHydro_weight_bilinear.nc  \n",
    "PET0.RegridWeightGen.Log  \n",
    "SCRIP_NLDAS_bilinear.nc  \n",
    "SCRIP_WRFHydro_bilinear.nc  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect the regridding scripts from NCAR. These can be found on the WRF-Hydro website: https://ral.ucar.edu/projects/wrf_hydro/regridding-scripts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect the regridding scripts\n",
    "\n",
    "# download the subset archive\n",
    "print('Downloading ESMF regridding scripts')\n",
    "archive_name = wget.download('https://ral.ucar.edu/sites/default/files/public/ESMFregrid_NLDAS.tar_.gz')\n",
    "\n",
    "# untar the archive\n",
    "print('Extracting archive contents')\n",
    "tar = tarfile.open(archive_name)\n",
    "tar.extractall()\n",
    "tar.close()\n",
    "\n",
    "# move the domain files into the DOMAIN directory\n",
    "print('Organizing domain data')\n",
    "extracted_folder = 'NLDAS'\n",
    "for f in glob.glob(os.path.join(extracted_folder, '*.ncl')):\n",
    "    shutil.move(f, os.getcwd())\n",
    "\n",
    "print('Cleaning up')\n",
    "os.remove(archive_name)\n",
    "shutil.rmtree(extracted_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the regridding weight file using the first NLDAS file in the `input_files` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$ncl_exec\"\n",
    "\n",
    "FILE=`find input_files -name 'NLDAS*' | head -1`\n",
    "\n",
    "$1 'interp_opt=\"bilinear\"' \\\n",
    "'srcGridName=\"'$FILE'\"' \\\n",
    "'dstGridName=\"DOMAIN/geo_em.d01.nc\"' \\\n",
    "NLDAS2WRFHydro_generate_weights.ncl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regrid all of the NLDAS files in the `input_files` directory using the NLDAS2WRFHydro_regrid.ncl script. This script takes NLDAS data and a weight file as inputs and outputs regridded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$ncl_exec\" \n",
    "\n",
    "$1 'srcFileName=\"NLDAS*\"' \\\n",
    "'dstGridName=\"DOMAIN/geo_em.d01.nc\"' \\\n",
    "NLDAS2WRFHydro_regrid.ncl > regrid.log 2> regrid.err\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the our directory by (1) removing the raw NLDAS data that we downloaded from EarthData and (2) rename the default `output_files` directory to `FORCING`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forcing_dir = 'FORCING'\n",
    "if os.path.exists(forcing_dir):\n",
    "    while 1:\n",
    "        res = input(\"Directory 'FORCING' already exists. Do you wish to remove it [Y/N]?\")\n",
    "        if res.lower() == 'y':\n",
    "            shutil.rmtree(forcing_dir)\n",
    "            os.mkdir(forcing_dir)\n",
    "            break\n",
    "        elif res.lower() == 'n':\n",
    "            break\n",
    "        else:\n",
    "            print('\\nInvalid input. Please answer either Y or N')\n",
    "else:\n",
    "    os.mkdir(forcing_dir)\n",
    "\n",
    "# move the regridded files into the FORCING directory\n",
    "print('Organizing FORCING data')\n",
    "output_dir = 'output_files'\n",
    "for f in glob.glob(os.path.join(output_dir, '*')):\n",
    "    shutil.move(f, forcing_dir)\n",
    "    \n",
    "print('Cleaning up')\n",
    "shutil.rmtree(output_dir)\n",
    "shutil.rmtree('input_files')\n",
    "regrid_files = []\n",
    "regrid_files.extend(glob.glob('NLDAS2WRFHydro*'))\n",
    "regrid_files.extend(glob.glob('SCRIP*'))\n",
    "regrid_files.extend(glob.glob('PET0*'))\n",
    "for f in regrid_files:\n",
    "    os.remove(f)\n",
    "    \n",
    "!du -h FORCING/*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
